{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "576037d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0457, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5393, 0.7695, 0.0000, 0.0000, 0.0000],\n",
       "        [0.8812, 0.0073, 0.8768, 0.0000, 0.0000],\n",
       "        [0.3503, 0.3577, 0.0549, 0.8055, 0.0000],\n",
       "        [0.3590, 0.1970, 0.9858, 0.1102, 0.1255]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from einops import reduce\n",
    "import numpy as np\n",
    "# 示例 2: 从向量创建对角矩阵 (返回2D tensor)\n",
    "a = torch.rand(5, 5)\n",
    "torch.tril(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1362e8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from einops import rearrange\n",
    "from einops import einsum\n",
    "import sys\n",
    "sys.path.append(\"/Users/bitkira/Documents/GitHub/Stanford-CS336/assignment1-basics-main/\")\n",
    "from cs336_basics.RMSnorm import RMSnorm\n",
    "from cs336_basics.SwiGLU import SwiGLU\n",
    "from cs336_basics.MHA import MultiheadSelfAttention\n",
    "from cs336_basics.Linear import linear\n",
    "from cs336_basics.Softmax import softmax\n",
    "from cs336_basics.RoPE import rope\n",
    "\n",
    "class rope(nn.Module):\n",
    "    def __init__(self, theta: float, d_k: int, max_seq_len: int, device=None):\n",
    "        super().__init__()\n",
    "        THETA = torch.tensor([math.pow(theta, (2*k)/d_k) for k in range(int(d_k/2))])\n",
    "\n",
    "        R_list = []\n",
    "        for j in range(max_seq_len):\n",
    "            R = np.zeros((d_k, d_k), dtype=np.float32)\n",
    "            R[0::2, 0::2] = np.diag(torch.cos(j/THETA))  \n",
    "            R[0::2, 1::2] = np.diag(-torch.sin(j/THETA))   \n",
    "            R[1::2, 0::2] = np.diag(torch.sin(j/THETA)) \n",
    "            R[1::2, 1::2] = np.diag(torch.cos(j/THETA))\n",
    "            R_list.append(R)\n",
    "        self.register_buffer(\"RoPE\" ,torch.tensor(R_list, dtype=torch.float32),persistent=False)\n",
    "    def forward(self, x: torch.Tensor, token_positions: torch.Tensor) -> torch.Tensor:\n",
    "        self.RoPE = self.RoPE[0:x.shape[-2], 0:x.shape[-2]]\n",
    "        print(\"self.RoPE\",x.shape,self.RoPE[token_positions].shape)\n",
    "        return einsum(x, self.RoPE[token_positions], \"... sequence_length d_k, ... sequence_length dk d_k-> ... sequence_length dk\")\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    k = rearrange(k, \"... seq_len d_k -> ... d_k seq_len\")\n",
    "    out = torch.matmul(q, k) / torch.sqrt(torch.tensor(q.shape[-1], dtype=q.dtype))\n",
    "    if mask is not None:\n",
    "        out.masked_fill_(~mask,float('-inf'))\n",
    "    return torch.matmul(softmax(out, i=-1), v)\n",
    "\n",
    "class MultiheadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, max_seq_len=None, theta=None, token_positions=None):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.Q = linear(d_model, d_model)\n",
    "        self.K = linear(d_model, d_model)\n",
    "        self.V = linear(d_model, d_model)\n",
    "        self.O = linear(d_model, d_model)\n",
    "        if theta is not None:\n",
    "            self.rope = rope(theta, d_model//num_heads, max_seq_len)\n",
    "            self.token_pos = token_positions\n",
    "        else:\n",
    "            self.rope = None\n",
    "    def forward(self, x):\n",
    "        mask = torch.tril(torch.ones(x.shape[1], x.shape[1], dtype=int)).bool()\n",
    "        Q = rearrange(self.Q(x), \"batch_size seq_len (h dk) -> batch_size h seq_len dk\", h=self.num_heads, dk=self.d_model//self.num_heads)\n",
    "        K = rearrange(self.K(x), \"batch_size seq_len (h dk) -> batch_size h seq_len dk\", h=self.num_heads, dk=self.d_model//self.num_heads)\n",
    "        if self.rope is not None:\n",
    "            Q = self.rope(Q, self.token_pos)\n",
    "            K = self.rope(K, self.token_pos)\n",
    "        V = rearrange(self.V(x), \"batch_size seq_len (h dv) -> batch_size h seq_len dv\", h=self.num_heads, dv=self.d_model//self.num_heads)\n",
    "\n",
    "        attention_score = scaled_dot_product_attention(Q, K, V, mask=mask)\n",
    "        attention_score = rearrange(attention_score, \"batch_size h seq_len dv -> batch_size seq_len (h dv)\")\n",
    "        return self.O(attention_score)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, max_seq_len, theta, token_pos):\n",
    "        super().__init__()\n",
    "        pos = torch.tensor([i for i in range(token_pos)], dtype=int)\n",
    "        print(type(max_seq_len))\n",
    "        self.norm1 = RMSnorm(d_model)\n",
    "        self.norm2 = RMSnorm(d_model)\n",
    "        self.MHA_layer = MultiheadSelfAttention(d_model, num_heads, max_seq_len, theta, pos)\n",
    "        self.SwiGLU = SwiGLU(d_model, d_ff)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        sub_result1 = x + self.MHA_layer(self.norm1(x))\n",
    "        sub_result2 = sub_result1 + self.SwiGLU(self.norm2(sub_result1))\n",
    "        return sub_result2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "110f2265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "a = TransformerBlock(64,4,128,16,1000,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "76be8fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.RoPE torch.Size([2, 4, 12, 16]) torch.Size([12, 12, 16])\n",
      "self.RoPE torch.Size([2, 4, 12, 16]) torch.Size([12, 12, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0043, 0.0998, 0.4859,  ..., 0.0446, 0.2757, 0.5248],\n",
       "         [0.1893, 0.4030, 0.5727,  ..., 0.1572, 0.1872, 0.3205],\n",
       "         [0.0764, 0.7939, 0.0839,  ..., 0.6664, 0.5288, 0.3886],\n",
       "         ...,\n",
       "         [0.2704, 0.8901, 0.3727,  ..., 0.2958, 0.8465, 0.1121],\n",
       "         [0.8617, 0.4179, 0.1906,  ..., 0.9061, 0.0991, 0.2995],\n",
       "         [0.6133, 0.4797, 0.2635,  ..., 0.3329, 0.6796, 0.1284]],\n",
       "\n",
       "        [[0.6101, 0.9964, 0.6968,  ..., 0.1056, 0.2436, 0.7811],\n",
       "         [0.0854, 0.1881, 0.2927,  ..., 0.2243, 0.5200, 0.0094],\n",
       "         [0.5552, 0.5403, 0.2444,  ..., 0.9025, 0.9709, 0.0321],\n",
       "         ...,\n",
       "         [0.0281, 0.3953, 0.8058,  ..., 0.9588, 0.2706, 0.5672],\n",
       "         [0.7009, 0.3475, 0.2917,  ..., 0.0375, 0.6552, 0.9332],\n",
       "         [0.6638, 0.9952, 0.8414,  ..., 0.8560, 0.3369, 0.4599]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a(torch.rand(2, 12, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f456ce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange, reduce\n",
    "import torch\n",
    "import sys\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sys.path.append(\"/Users/bitkira/Documents/GitHub/Stanford-CS336/assignment1-basics-main/\")\n",
    "from cs336_basics.Softmax import softmax\n",
    "\n",
    "def CrossEntropy(logits ,targts):\n",
    "    logits = softmax(logits, i=-1)\n",
    "    print(logits.shape)\n",
    "    num_classes = logits.shape[-1]\n",
    "    print(num_classes)\n",
    "    targts = F.one_hot(targts, num_classes).bool()\n",
    "    logits.masked_fill_(~targts, value=0)\n",
    "    logits = torch.log(logits)\n",
    "    return reduce(logits, \"batchsizeseqlen vocabsize -> 1\", \"mean\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "36f3e506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1000])\n",
      "1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-inf])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "b = torch.randint(100, (4,))\n",
    "a = torch.rand(4,1000)\n",
    "CrossEntropy(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b856cf04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7, 22,  9, 77])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(100, (4,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fadf4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange, reduce\n",
    "import torch\n",
    "import sys\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sys.path.append(\"/Users/bitkira/Documents/GitHub/Stanford-CS336/assignment1-basics-main/\")\n",
    "from cs336_basics.Softmax import softmax\n",
    "\n",
    "def CrossEntropy(logits ,targts):\n",
    "    num_classes = logits.shape[-1]\n",
    "    targts = F.one_hot(targts, num_classes).bool()\n",
    "    print(targts)\n",
    "    max, _ = torch.max(logits, dim=-1)\n",
    "    print(torch.exp(logits - torch.max(logits, dim=-1, keepdim=True).values))\n",
    "    expsum = reduce(torch.exp(logits - torch.max(logits, dim=-1, keepdim=True).values), \"logits a -> logits\", \"sum\")\n",
    "    print(expsum)\n",
    "    logits = logits[targts]\n",
    "    print(logits)\n",
    "    logits = -logits + max +torch.log(expsum)\n",
    "    print(logits)\n",
    "\n",
    "    return reduce(logits, \"logits -> 1\", \"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e94583d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False,  True, False, False, False, False, False, False, False],\n",
      "        [ True, False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False,  True, False, False],\n",
      "        [False, False, False, False, False,  True, False, False, False, False]])\n",
      "tensor([[0.4740, 0.7134, 1.0000, 0.6530, 0.9939, 0.6072, 0.4905, 0.5907, 0.4907,\n",
      "         0.9025],\n",
      "        [0.4408, 0.6923, 0.4719, 0.4512, 0.4713, 0.6449, 0.4366, 0.7188, 0.8781,\n",
      "         1.0000],\n",
      "        [0.9887, 0.6864, 0.6747, 0.5528, 0.4175, 1.0000, 0.5254, 0.4352, 0.6297,\n",
      "         0.4218],\n",
      "        [0.9957, 0.4549, 0.6248, 0.5047, 1.0000, 0.6221, 0.7162, 0.5492, 0.6640,\n",
      "         0.6000]])\n",
      "tensor([6.9159, 6.2059, 6.3322, 6.7317])\n",
      "tensor([0.7600, 0.0132, 0.0967, 0.4656])\n",
      "tensor([1.9338, 2.6447, 2.6777, 2.3814])\n",
      "CrossEntropy Loss: tensor([2.4094])\n"
     ]
    }
   ],
   "source": [
    "# 创建测试数据\n",
    "logits = torch.rand(4, 10)  # 假设有4个样本，每个样本有10个类别的logits\n",
    "targets = torch.randint(0, 10, (4,))  # 每个样本的目标类别\n",
    "\n",
    "# 调用CrossEntropy函数\n",
    "loss = CrossEntropy(logits, targets)\n",
    "print(\"CrossEntropy Loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "65cb98d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1 Linear(in_features=10, out_features=32, bias=True)\n",
      "activation ReLU()\n",
      "layer2 Linear(in_features=32, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict # state_dict 返回的是 OrderedDict\n",
    "\n",
    "# 定义一个简单的测试模型\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_features, output_features):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(input_features, 32,)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(32, output_features)\n",
    "        # 添加一个不需要梯度的参数\n",
    "        self.frozen_param = nn.Parameter(torch.randn(5),)\n",
    "        self.frozen_param2 = nn.Parameter(torch.randn((100,22)), requires_grad=False)\n",
    "        self.frozen_param3 = nn.Parameter(torch.randn(5), requires_grad=False)\n",
    "        # 添加一个 buffer\n",
    "        self.register_buffer('my_buffer', torch.randn(3))\n",
    "        # 添加一个普通的 Tensor 属性 (不会出现在任何一个里面)\n",
    "        self.normal_tensor_attr = torch.randn(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "# 实例化模型\n",
    "model1 = SimpleModel(10, 1)\n",
    "\n",
    "for i, l in model1.named_children():\n",
    "    print(i, l)\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898d2931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def DataLoader(x, batch_size, context_length, device_string):\n",
    "    inputtuple = []\n",
    "    labeltuple = []\n",
    "    for i in range(len(x)-context_length-1):\n",
    "        inputtuple.append(x[i:i+context_length+1][0:context_length])\n",
    "        labeltuple.append(x[i:i+context_length+1][1:context_length+1])\n",
    "    batchinput = []\n",
    "    batchlabel = []\n",
    "    for j in range(batch_size):\n",
    "        batchinput.append(inputtuple[j])\n",
    "        batchlabel.append(labeltuple[j])\n",
    "    return (torch.tensor(batchinput, device=device_string), torch.tensor(batchlabel, device=device_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6cffa897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 7])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array(range(100))\n",
    "b = 32\n",
    "c = 7\n",
    "d = \"mps\"\n",
    "a,_ = DataLoader(x, b, c, d)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c985c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transformer_instance_small' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(\u001b[43mtransformer_instance_small\u001b[49m))\n",
      "\u001b[31mNameError\u001b[39m: name 'transformer_instance_small' is not defined"
     ]
    }
   ],
   "source": [
    "from cs336_basics.TransformerLM import Transformer\n",
    "transformer_instance_large = Transformer(\n",
    "    d_model=16,          # 模型维度 (一个较小的值)\n",
    "    num_heads=2,        # 注意力头数 (d_model 必须能被 num_heads 整除, 8 % 2 == 0)\n",
    "    d_ff=64,             # 前馈神经网络的隐藏层维度 (可以设为 d_model 或其倍数，这里设为 d_model)\n",
    "    theta=10000.0,      # RoPE (旋转位置编码) 的参数 theta，通常是 10000.0\n",
    "    token_pos=16,       # 这个参数的含义不是很明确，假设它代表最大位置编码数，与 context_length 一致\n",
    "    vocab_size=32,      # 词汇表大小 (一个较小的值)\n",
    "    context_length=16,  # 上下文长度/最大序列长度 (一个较小的值)\n",
    "    num_layers=4        # Transformer 块的层数 (最小为 1)\n",
    ")\n",
    "transformer_instance_large.to(device=\"mps\")\n",
    "print(str(transformer_instance_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21afca4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MacPytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
