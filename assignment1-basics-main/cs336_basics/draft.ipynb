{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "576037d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0457, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5393, 0.7695, 0.0000, 0.0000, 0.0000],\n",
       "        [0.8812, 0.0073, 0.8768, 0.0000, 0.0000],\n",
       "        [0.3503, 0.3577, 0.0549, 0.8055, 0.0000],\n",
       "        [0.3590, 0.1970, 0.9858, 0.1102, 0.1255]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from einops import reduce\n",
    "import numpy as np\n",
    "# 示例 2: 从向量创建对角矩阵 (返回2D tensor)\n",
    "a = torch.rand(5, 5)\n",
    "torch.tril(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1362e8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from einops import rearrange\n",
    "from einops import einsum\n",
    "import sys\n",
    "sys.path.append(\"/Users/bitkira/Documents/GitHub/Stanford-CS336/assignment1-basics-main/\")\n",
    "from cs336_basics.RMSnorm import RMSnorm\n",
    "from cs336_basics.SwiGLU import SwiGLU\n",
    "from cs336_basics.MHA import MultiheadSelfAttention\n",
    "from cs336_basics.Linear import linear\n",
    "from cs336_basics.Softmax import softmax\n",
    "from cs336_basics.RoPE import rope\n",
    "\n",
    "class rope(nn.Module):\n",
    "    def __init__(self, theta: float, d_k: int, max_seq_len: int, device=None):\n",
    "        super().__init__()\n",
    "        THETA = torch.tensor([math.pow(theta, (2*k)/d_k) for k in range(int(d_k/2))])\n",
    "\n",
    "        R_list = []\n",
    "        for j in range(max_seq_len):\n",
    "            R = np.zeros((d_k, d_k), dtype=np.float32)\n",
    "            R[0::2, 0::2] = np.diag(torch.cos(j/THETA))  \n",
    "            R[0::2, 1::2] = np.diag(-torch.sin(j/THETA))   \n",
    "            R[1::2, 0::2] = np.diag(torch.sin(j/THETA)) \n",
    "            R[1::2, 1::2] = np.diag(torch.cos(j/THETA))\n",
    "            R_list.append(R)\n",
    "        self.register_buffer(\"RoPE\" ,torch.tensor(R_list, dtype=torch.float32),persistent=False)\n",
    "    def forward(self, x: torch.Tensor, token_positions: torch.Tensor) -> torch.Tensor:\n",
    "        self.RoPE = self.RoPE[0:x.shape[-2], 0:x.shape[-2]]\n",
    "        print(\"self.RoPE\",x.shape,self.RoPE[token_positions].shape)\n",
    "        return einsum(x, self.RoPE[token_positions], \"... sequence_length d_k, ... sequence_length dk d_k-> ... sequence_length dk\")\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    k = rearrange(k, \"... seq_len d_k -> ... d_k seq_len\")\n",
    "    out = torch.matmul(q, k) / torch.sqrt(torch.tensor(q.shape[-1], dtype=q.dtype))\n",
    "    if mask is not None:\n",
    "        out.masked_fill_(~mask,float('-inf'))\n",
    "    return torch.matmul(softmax(out, i=-1), v)\n",
    "\n",
    "class MultiheadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, max_seq_len=None, theta=None, token_positions=None):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.Q = linear(d_model, d_model)\n",
    "        self.K = linear(d_model, d_model)\n",
    "        self.V = linear(d_model, d_model)\n",
    "        self.O = linear(d_model, d_model)\n",
    "        if theta is not None:\n",
    "            self.rope = rope(theta, d_model//num_heads, max_seq_len)\n",
    "            self.token_pos = token_positions\n",
    "        else:\n",
    "            self.rope = None\n",
    "    def forward(self, x):\n",
    "        mask = torch.tril(torch.ones(x.shape[1], x.shape[1], dtype=int)).bool()\n",
    "        Q = rearrange(self.Q(x), \"batch_size seq_len (h dk) -> batch_size h seq_len dk\", h=self.num_heads, dk=self.d_model//self.num_heads)\n",
    "        K = rearrange(self.K(x), \"batch_size seq_len (h dk) -> batch_size h seq_len dk\", h=self.num_heads, dk=self.d_model//self.num_heads)\n",
    "        if self.rope is not None:\n",
    "            Q = self.rope(Q, self.token_pos)\n",
    "            K = self.rope(K, self.token_pos)\n",
    "        V = rearrange(self.V(x), \"batch_size seq_len (h dv) -> batch_size h seq_len dv\", h=self.num_heads, dv=self.d_model//self.num_heads)\n",
    "\n",
    "        attention_score = scaled_dot_product_attention(Q, K, V, mask=mask)\n",
    "        attention_score = rearrange(attention_score, \"batch_size h seq_len dv -> batch_size seq_len (h dv)\")\n",
    "        return self.O(attention_score)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, max_seq_len, theta, token_pos):\n",
    "        super().__init__()\n",
    "        pos = torch.tensor([i for i in range(token_pos)], dtype=int)\n",
    "        print(type(max_seq_len))\n",
    "        self.norm1 = RMSnorm(d_model)\n",
    "        self.norm2 = RMSnorm(d_model)\n",
    "        self.MHA_layer = MultiheadSelfAttention(d_model, num_heads, max_seq_len, theta, pos)\n",
    "        self.SwiGLU = SwiGLU(d_model, d_ff)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        sub_result1 = x + self.MHA_layer(self.norm1(x))\n",
    "        sub_result2 = sub_result1 + self.SwiGLU(self.norm2(sub_result1))\n",
    "        return sub_result2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "110f2265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "a = TransformerBlock(64,4,128,16,1000,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "76be8fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.RoPE torch.Size([2, 4, 12, 16]) torch.Size([12, 12, 16])\n",
      "self.RoPE torch.Size([2, 4, 12, 16]) torch.Size([12, 12, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0043, 0.0998, 0.4859,  ..., 0.0446, 0.2757, 0.5248],\n",
       "         [0.1893, 0.4030, 0.5727,  ..., 0.1572, 0.1872, 0.3205],\n",
       "         [0.0764, 0.7939, 0.0839,  ..., 0.6664, 0.5288, 0.3886],\n",
       "         ...,\n",
       "         [0.2704, 0.8901, 0.3727,  ..., 0.2958, 0.8465, 0.1121],\n",
       "         [0.8617, 0.4179, 0.1906,  ..., 0.9061, 0.0991, 0.2995],\n",
       "         [0.6133, 0.4797, 0.2635,  ..., 0.3329, 0.6796, 0.1284]],\n",
       "\n",
       "        [[0.6101, 0.9964, 0.6968,  ..., 0.1056, 0.2436, 0.7811],\n",
       "         [0.0854, 0.1881, 0.2927,  ..., 0.2243, 0.5200, 0.0094],\n",
       "         [0.5552, 0.5403, 0.2444,  ..., 0.9025, 0.9709, 0.0321],\n",
       "         ...,\n",
       "         [0.0281, 0.3953, 0.8058,  ..., 0.9588, 0.2706, 0.5672],\n",
       "         [0.7009, 0.3475, 0.2917,  ..., 0.0375, 0.6552, 0.9332],\n",
       "         [0.6638, 0.9952, 0.8414,  ..., 0.8560, 0.3369, 0.4599]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a(torch.rand(2, 12, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f456ce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange, reduce\n",
    "import torch\n",
    "import sys\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sys.path.append(\"/Users/bitkira/Documents/GitHub/Stanford-CS336/assignment1-basics-main/\")\n",
    "from cs336_basics.Softmax import softmax\n",
    "\n",
    "def CrossEntropy(logits ,targts):\n",
    "    logits = softmax(logits, i=-1)\n",
    "    print(logits.shape)\n",
    "    num_classes = logits.shape[-1]\n",
    "    print(num_classes)\n",
    "    targts = F.one_hot(targts, num_classes).bool()\n",
    "    logits.masked_fill_(~targts, value=0)\n",
    "    logits = torch.log(logits)\n",
    "    return reduce(logits, \"batchsizeseqlen vocabsize -> 1\", \"mean\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "36f3e506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1000])\n",
      "1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-inf])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "b = torch.randint(100, (4,))\n",
    "a = torch.rand(4,1000)\n",
    "CrossEntropy(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b856cf04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7, 22,  9, 77])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(100, (4,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fadf4df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MacPytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
